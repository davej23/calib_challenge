{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['labeled/3.hevc',\n",
       " 'labeled/0.hevc',\n",
       " 'labeled/4.hevc',\n",
       " 'labeled/2.hevc',\n",
       " 'labeled/1.hevc']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelled_files_video = [f\"labeled/{n}\" for n in os.listdir(\"labeled\") if \".hevc\" in n ]\n",
    "labelled_files_video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['labeled/3.txt',\n",
       " 'labeled/0.txt',\n",
       " 'labeled/4.txt',\n",
       " 'labeled/2.txt',\n",
       " 'labeled/1.txt']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelled_files_label = [n.replace('.hevc', '.txt') for n in labelled_files_video]\n",
    "labelled_files_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['unlabeled/5.hevc',\n",
       " 'unlabeled/6.hevc',\n",
       " 'unlabeled/9.hevc',\n",
       " 'unlabeled/8.hevc',\n",
       " 'unlabeled/7.hevc']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabelled_files_video = [f\"unlabeled/{n}\" for n in os.listdir(\"unlabeled\") if \".hevc\" in n ]\n",
    "unlabelled_files_video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SHAPE = (256, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 256, 256]), torch.Size([1, 2]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def data_loader(ds=\"train\"):\n",
    "    if ds == \"train\":\n",
    "        for video_file, label_file in zip(labelled_files_video, labelled_files_label):\n",
    "            vid = cv2.VideoCapture(video_file)\n",
    "            lab = torch.from_numpy(np.nan_to_num(np.loadtxt(label_file))).unsqueeze(0).float()\n",
    "            idx = 0\n",
    "            while True:\n",
    "                next_frame_found, frame = vid.read()\n",
    "                if not next_frame_found:\n",
    "                    break\n",
    "                frame = cv2.resize(frame, IMAGE_SHAPE)\n",
    "                image = torch.from_numpy(np.array(frame, np.float32) / 255.0).permute(2, 0, 1).unsqueeze(0)\n",
    "                label = lab[:, idx, :]\n",
    "                idx += 1\n",
    "                yield image, label\n",
    "\n",
    "    else:\n",
    "        for video_file in unlabelled_files_video:\n",
    "            vid = cv2.VideoCapture(video_file)\n",
    "            while True:\n",
    "                next_frame_found, frame = vid.read()\n",
    "                if not next_frame_found:\n",
    "                    break\n",
    "                frame = cv2.resize(frame, IMAGE_SHAPE)\n",
    "                image = torch.from_numpy(np.array(frame, np.float32) / 255.0).permute(2, 0, 1).unsqueeze(0)\n",
    "                yield image, None\n",
    "\n",
    "next(data_loader())[0].shape, next(data_loader())[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = torch.nn.Conv2d(3, 8, 2, 2)\n",
    "        self.l2 = torch.nn.Conv2d(8, 16, 2, 2)\n",
    "        self.l3 = torch.nn.ConvTranspose2d(16, 8, 2, 2)\n",
    "        self.l4 = torch.nn.ConvTranspose2d(8, 1, 2, 2)\n",
    "        self.l5 = torch.nn.Flatten()\n",
    "        self.l6 = torch.nn.Linear(IMAGE_SHAPE[0] * IMAGE_SHAPE[1], 32)\n",
    "        self.l7 = torch.nn.Linear(32, 2)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.l1(x)\n",
    "        x = self.l2(x)\n",
    "        x = self.l3(x)\n",
    "        x = self.l4(x)\n",
    "        x = self.l5(x)\n",
    "        x = self.l6(x)\n",
    "        x = self.l7(x)\n",
    "        return x\n",
    "\n",
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter count:  2.098435  million\n"
     ]
    }
   ],
   "source": [
    "print(\"Parameter count: \", sum((p.numel() for p in model.parameters() if p.requires_grad)) / 1e6 , \" million\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss epoch 0  --  0.04395794868469238\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 1\n",
    "optim = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_data = data_loader(\"train\")\n",
    "    model.train()\n",
    "    for image, label in train_data:\n",
    "        optim.zero_grad()\n",
    "        out = model(image)\n",
    "        loss = loss_fn(out, label)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "    model.eval()\n",
    "    train_data = data_loader(\"train\")\n",
    "    train_loss = 0.0\n",
    "    count = 0\n",
    "    with torch.no_grad():\n",
    "        for image, label in train_data:\n",
    "            out = model(image)\n",
    "            loss = loss_fn(out, label)\n",
    "            train_loss += loss\n",
    "            count += 1\n",
    "\n",
    "    print(f\"loss epoch {epoch}  --  {train_loss / count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"best-model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get predictions on validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data = data_loader(ds=\"valid\")\n",
    "valid_data_preds = []\n",
    "file_idx = 0\n",
    "for idx, (image, _) in enumerate(valid_data):\n",
    "    valid_data_preds.append(model(image).detach().numpy())\n",
    "\n",
    "    if len(valid_data_preds) == 1200:  # reset every 1200 predictions\n",
    "        predictions = np.concatenate(valid_data_preds, 0)\n",
    "        np.savetxt(unlabelled_files_video[file_idx].replace(\"hevc\", \"txt\"), predictions)\n",
    "        file_idx += 1\n",
    "        valid_data_preds = []\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
